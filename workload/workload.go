package workload

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"log/slog"
	"math/rand"
	"net/http"
	"os"
	"os/signal"
	"sync"
	"syscall"
	"time"

	gotel "github.com/couchbase/gocb-opentelemetry"
	"github.com/couchbase/gocb/v2"
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promhttp"
	"go.opentelemetry.io/otel/attribute"
	"go.uber.org/zap"
)

// A spectroperf Workload is defined by:
//   - The documents it operates on
//   - The Operations it performs on those documents
//   - The probability matrix defining the likelihood of one operation being followed by another
type Workload interface {
	// GenerateDocument creates a random document appropriate for the workload
	GenerateDocument(id string) DocType
	// Operations returns The list of operations that a workload supports
	Operations() []string
	// Probabilities returns the probability matrix for the workload
	Probabilities() [][]float64
	// Returns a map of operations to workload functions
	Functions() map[string]func(ctx context.Context, rctx Runctx) error
	// Setup performs any workload specific setup, e.g creating indexes
	Setup() error
}

// InitMetrics initialises the metrics labelled with the operations performed by the given workload
func InitMetrics(w Workload) {
	// Create a non-global registry.
	reg := prometheus.NewRegistry()
	reg.MustRegister(opsAttempted)
	reg.MustRegister(opsFailed)
	reg.MustRegister(opDuration)

	// Setup metrics
	for _, operation := range w.Operations() {
		attemptMetrics[operation] = map[OperationPhase]prometheus.Counter{}
		failedMetrics[operation] = map[OperationPhase]prometheus.Counter{}
		durationMetrics[operation] = map[OperationPhase]prometheus.Observer{}
		for _, phase := range States {
			attemptMetrics[operation][phase] = opsAttempted.WithLabelValues(operation, string(phase))
			failedMetrics[operation][phase] = opsFailed.WithLabelValues(operation, string(phase))
			durationMetrics[operation][phase] = opDuration.WithLabelValues(operation, string(phase))
		}
	}

	// Expose metrics and custom registry via an HTTP server
	go func() {
		http.Handle("/metrics", promhttp.HandlerFor(reg, promhttp.HandlerOpts{Registry: reg}))
		log.Fatal(http.ListenAndServe(":2112", nil))
	}()
}

// Setup uploads the documents generated by the workload, and calls the workloads Setup function
func Setup(w Workload, logger *zap.Logger, numItemsArg int, scp *gocb.Scope, coll *gocb.Collection) {
	numConc := 2000
	workChan := make(chan DocType, numConc)
	shutdownChan := make(chan struct{}, numConc)
	var wg sync.WaitGroup

	logger.Info("Inserting documents", zap.Int("number of docs", numItemsArg))
	wg.Add(numConc)
	for i := 0; i < numConc; i++ {
		go func() {
			for {
				select {
				case doc := <-workChan:
					_, err := coll.Upsert(doc.Name, doc.Data, nil)
					if err != nil {
						logger.Fatal("data load upsert failed", zap.Error(err))
					}
				case <-shutdownChan:
					wg.Done()
					return
				}
			}
		}()
	}

	// Create a random document using the given workload definition
	for i := 0; i < numItemsArg; i++ {
		workChan <- w.GenerateDocument(fmt.Sprintf("u%d", i))
	}

	// Call the workload's own Setup function to perform any workload specific setup
	err := w.Setup()
	if err != nil {
		logger.Fatal("failed to setup workload", zap.Error(err))
	}
}

func Run(
	w Workload,
	logger *zap.Logger,
	mChain [][]float64,
	numUsers int,
	runTime time.Duration,
	rampTime time.Duration,
	tracer *gotel.OpenTelemetryRequestTracer,
	sleep time.Duration,
) {
	sigCh := make(chan os.Signal, 10)
	ctx, cancelFn := context.WithCancel(context.Background())

	go func() {
		<-sigCh
		cancelFn()
	}()

	// Signal handler for SIGTERM
	signal.Notify(sigCh, os.Interrupt, syscall.SIGTERM)

	// Create a work group of goroutine runners sharing the same probabilities.
	var wg sync.WaitGroup

	wg.Add(numUsers)
	for i := 0; i < numUsers; i++ {
		go runLoop(ctx, logger, mChain, sleep, w.Functions(), w.Operations(), runTime, rampTime, i, &wg, tracer)
	}

	wg.Wait()

	if !PrometheusIsRunning() {
		logger.Info("skipping writing metrics to file as prometheus is not running")
		return
	}

	logger.Info("scraping operation metrics from prometheus to write to file")

	// Add a minute onto the range to make sure none of the metrics are missed.
	timeRange := int(runTime.Minutes()) + 1
	metricSummaries := map[string]OperationSummary{}
	for _, op := range w.Operations() {
		summary, err := SummariseOperationMetrics(op, timeRange)
		if err != nil {
			logger.Info("skipping operation due to error", zap.Error(err), zap.String("operation", op))
			continue
		}

		metricSummaries[op] = *summary
	}

	bytes, err := json.Marshal(metricSummaries)
	if err != nil {
		logger.Fatal("marshalling metric summary", zap.Error(err), zap.Any("summary", metricSummaries))
	}

	timeStamp := time.Now().UTC().Format(time.RFC3339)
	filePath := fmt.Sprintf("%s.json", timeStamp)
	if err := os.WriteFile(filePath, bytes, 0644); err != nil {
		logger.Fatal("writing metric summary to file", zap.Error(err), zap.String("path", filePath))
	}
}

func runLoop(
	ctx context.Context,
	logger *zap.Logger,
	probabilities [][]float64,
	sleep time.Duration,
	functions map[string]func(context.Context, Runctx) error,
	operations []string,
	runTime time.Duration,
	rampTime time.Duration,
	runnerId int,
	wg *sync.WaitGroup,
	tracer *gotel.OpenTelemetryRequestTracer,
) {
	// Current operation index
	currOpIndex := 0

	rng := rand.NewSource(int64(RandSeed + runnerId))
	r := rand.New(rng)

	timeout := time.After(runTime)

	logger.Debug("Starting runner", zap.Int("runnerId", runnerId))
	runStart := time.Now()
	runEnd := runStart.Add(runTime)

	var runCtx Runctx
	runCtx.r = *r
	runCtx.l = *logger
	// todo: move this into context.value, a KV store for junk

	for {
		select {
		case <-ctx.Done():
			logger.Debug("Received cancel, stopping runner", zap.Int("runnerId", runnerId)) // TODO: fix bug where only one runner stops
			wg.Done()
		case <-timeout:
			logger.Debug("Run time reached, stopping runner", zap.Int("runnerId", runnerId))
			wg.Done()
		default:
			// Get the next operation index based on probabilities
			nextOpIndex := getNextOperation(currOpIndex, probabilities, r)
			// call the next function
			nextFunction := operations[nextOpIndex]
			slog.Debug(nextFunction)

			var t time.Duration
			if sleep == 0 {
				// sleep a random amount of time up to 5 seconds
				t = time.Duration(r.Int31n(5000-400)+400) * time.Millisecond
			} else {
				t = sleep
			}
			time.Sleep(t)

			phase := MetricState(runStart, runEnd, rampTime)
			attemptMetrics[nextFunction][phase].Inc()

			ctx2, span := tracer.Wrapped().Start(ctx, nextFunction)
			span.SetAttributes(attribute.Key("workload-phase").String(string(phase)))

			start := time.Now()
			err := functions[operations[nextOpIndex]](ctx2, runCtx)
			duration := time.Now().Sub(start)
			durationMetrics[nextFunction][phase].Observe(float64(duration.Microseconds()) / 1000)

			if err != nil {
				logger.Error("operation failed", zap.String("operation", nextFunction), zap.Error(err))
				failedMetrics[nextFunction][phase].Inc()
			}

			// update for next time
			currOpIndex = nextOpIndex
			span.End()
		}
	}
}

func getNextOperation(currOpIndex int, probabilities [][]float64, r *rand.Rand) int {
	// Get the probabilities for the current operation
	probRow := probabilities[currOpIndex]

	// Calculate a random value between 0 and the total probability (which should sum to 1)
	randVal := r.Float64()

	// Iterate through the probability row to select the next operation
	var cumulativeProb float64
	for i, prob := range probRow {
		cumulativeProb += prob
		if randVal < cumulativeProb {
			return i
		}
	}

	// Fallback: return the last operation if something goes wrong
	return len(probRow) - 1
}
